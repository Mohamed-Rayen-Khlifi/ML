{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Random Forest\n#### Randomly choose a subset of the features AND randomly choose a subset of the training examples to train each individual tree\n#### All of the hyperparameters found in the decision tree model will also exist in this algorithm, since a random forest is an ensemble of many Decision Trees\n#### One additional hyperparameter for Random Forest is called n_estimators which is the number of Decision Trees that make up the Random Forest\n#### If N is the number of features, we will randomly select  sqrt(N) of these features to train each individual tree (Can be modified by setting the max_features parameter)\n#### You can also speed up your training jobs with another parameter, n_jobs, since the fitting of each tree is independent of each other, it is possible fit more than one tree in parallel,  setting n_jobs higher will increase how many CPU cores it will use, Changing this parameter does not impact on the final result but can reduce the training time.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# The random forest uses many trees, and it makes a prediction by averaging the predictions of each component tree\n# It generally has much better predictive accuracy than a single decision tree and it works well with default parameters \n# But one of the best features of Random Forest models is that they generally work reasonably even without any tuning\n\nforest_model = random_forest_model = RandomForestClassifier(n_estimators = 100,\n                                             max_depth = 16, \n                                             min_samples_split = 10).fit(X_train,y_train)\n\n# Same as we did to the Decision Tree, to figure out the best hyper arameteres to use\nmin_samples_split_list = [2,10, 30, 50, 100, 200, 300, 700]  ## If the number is an integer, then it is the actual quantity of samples,\n                                             ## If it is a float, then it is the percentage of the dataset\nmax_depth_list = [2, 4, 8, 16, 32, 64, None]\nn_estimators_list = [10,50,100,500]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# min_samples_split hyper parameter \naccuracy_list_train = []\naccuracy_list_val = []\nfor min_samples_split in min_samples_split_list:\n    # You can fit the model at the same time you define it, because the fit function returns the fitted estimator.\n    model = RandomForestClassifier(min_samples_split = min_samples_split,\n                                   random_state = RANDOM_STATE).fit(X_train,y_train) \n    predictions_train = model.predict(X_train) ## The predicted values for the train dataset\n    predictions_val = model.predict(X_val) ## The predicted values for the test dataset\n    accuracy_train = accuracy_score(predictions_train,y_train)\n    accuracy_val = accuracy_score(predictions_val,y_val)\n    accuracy_list_train.append(accuracy_train)\n    accuracy_list_val.append(accuracy_val)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    \n# min_depth hyper parameter \naccuracy_list_train = []\naccuracy_list_val = []\nfor max_depth in max_depth_list:\n    # You can fit the model at the same time you define it, because the fit function returns the fitted estimator.\n    model = RandomForestClassifier(max_depth = max_depth,\n                                   random_state = RANDOM_STATE).fit(X_train,y_train) \n    predictions_train = model.predict(X_train) ## The predicted values for the train dataset\n    predictions_val = model.predict(X_val) ## The predicted values for the test dataset\n    accuracy_train = accuracy_score(predictions_train,y_train)\n    accuracy_val = accuracy_score(predictions_val,y_val)\n    accuracy_list_train.append(accuracy_train)\n    accuracy_list_val.append(accuracy_val)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# n_estimators hyper parameter \naccuracy_list_train = []\naccuracy_list_val = []\nfor n_estimators in n_estimators_list:\n    # You can fit the model at the same time you define it, because the fit function returns the fitted estimator.\n    model = RandomForestClassifier(n_estimators = n_estimators,\n                                   random_state = RANDOM_STATE).fit(X_train,y_train) \n    predictions_train = model.predict(X_train) ## The predicted values for the train dataset\n    predictions_val = model.predict(X_val) ## The predicted values for the test dataset\n    accuracy_train = accuracy_score(predictions_train,y_train)\n    accuracy_val = accuracy_score(predictions_val,y_val)\n    accuracy_list_train.append(accuracy_train)\n    accuracy_list_val.append(accuracy_val)\n    \naccuracy_score(random_forest_model.predict(X_train),y_train)\naccuracy_score(random_forest_model.predict(X_val),y_val)","metadata":{},"execution_count":null,"outputs":[]}]}