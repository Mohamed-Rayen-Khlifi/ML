{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.tree import DecisionTreeRegressor\n\n### Controlling Tree Depth\n# The max_leaf_nodes argument provides a very sensible way to control overfitting vs underfitting \n# The more leaves we allow the model to make, the more we move towards the overfitting\n# We can use a utility function to help compare MAE scores from different values for max_leaf_nodes\n\ndef get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    return(mae)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Code you have previously used to load data\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\n\n# Path of the file to read\niowa_file_path = '../input/home-data-for-ml-course/train.csv'\n\nhome_data = pd.read_csv(iowa_file_path)\n\n\n# Create target object and call it y\ny = home_data.SalePrice\n\n# Create X\nfeatures = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\nX = home_data[features]\n\n# Split into validation and training data\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hyper paramaters\nmin_samples_split_list = [2,10, 30, 50, 100, 200, 300, 700] # If the number is an integer, then it is the actual quantity of samples,\n                                                            # Choosing a higher min_samples_split can reduce the number of splits and may help to reduce overfitting\n    \n# Since we passed hyper parameters as lists, we are going to iterate over the values to find the best accuracy and less overfitting value\n# Experimenting with min_sample_split hyperparamter\naccuracy_list_train = []\naccuracy_list_val = []\nfor min_samples_split in min_samples_split_list: # Increasing the the number of min_samples_split reduces overfitting, \n    # Even though it does not improve the validation accuracy, it brings the training accuracy closer to it, showing a reduction in overfitting\n    # You can fit the model at the same time you define it, because the fit function returns the fitted estimator.\n    model = DecisionTreeClassifier(min_samples_split = min_samples_split,\n                                   random_state = RANDOM_STATE).fit(X_train,y_train) \n    predictions_train = model.predict(X_train) ## The predicted values for the train dataset\n    predictions_val = model.predict(X_val) ## The predicted values for the test dataset\n    accuracy_train = accuracy_score(predictions_train,y_train)\n    accuracy_val = accuracy_score(predictions_val,y_val)\n    accuracy_list_train.append(accuracy_train)\n    accuracy_list_val.append(accuracy_val)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_depth_list = [1,2, 3, 4, 8, 16, 32, 64, None] # None means that there is no depth limit.\n                                                  # Choosing a lower max_depth can reduce the number of splits and may help to reduce overfitting\n# Experimenting with max_depth hyperparamter\naccuracy_list_train = []\naccuracy_list_val = []\nfor max_depth in max_depth_list:\n    # You can fit the model at the same time you define it, because the fit function returns the fitted estimator.\n    model = DecisionTreeClassifier(max_depth = max_depth,\n                                   random_state = RANDOM_STATE).fit(X_train,y_train) \n    predictions_train = model.predict(X_train) ## The predicted values for the train dataset\n    predictions_val = model.predict(X_val) ## The predicted values for the test dataset\n    accuracy_train = accuracy_score(predictions_train,y_train)\n    accuracy_val = accuracy_score(predictions_val,y_val)\n    accuracy_list_train.append(accuracy_train)\n    accuracy_list_val.append(accuracy_val)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Underfitting: Both training and validation accuracy decreases; the tree cannot make enough splits to distinguish positives from negatives (the model is underfitting the training set)\n# Good parameters: Increases validation accuracy closer to training accuracy, even if it significantly reduces training accuracy and the validation accuracy reaches its highest at these paramteres\n# Overfitting: Validation accuracy decreases while training accuracy increases\n\n# After this expirment, we can chose the correct and most accurate paramteres for our Tree \ndecision_tree_model = DecisionTreeClassifier(min_samples_split = 50,\n                                             max_depth = 3,\n                                             random_state = RANDOM_STATE).fit(X_train,y_train)\n\n                                                                              \naccuracy_score(decision_tree_model.predict(X_train),y_train)\naccuracy_score(decision_tree_model.predict(X_val),y_val)\n               ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that we are searching for the best value one hyperparameter while leaving the other hyperparameters at their default values.\n- Ideally, we would want to check every combination of values for every hyperparameter that we are tuning.\n- If we have 3 hyperparameters, and each hyperparameter has 4 values to try out, we should have a total of 4 x 4 x 4 = 64 combinations to try.\n- When we only modify one hyperparameter while leaving the rest as their default value, we are trying 4 + 4 + 4 = 12 results. \n- To try out all combinations, we can use a sklearn implementation called GridSearchCV. GridSearchCV has a refit parameter that will automatically refit a model on the best combination so we will not need to program it explicitly. For more on GridSearchCV, please refer to its [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}