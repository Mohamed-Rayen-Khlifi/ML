{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# for array computations and loading data\nimport numpy as np\n\n# for building linear regression models and preparing data\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n# for building and training neural networks\nimport tensorflow as tf\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Splitting the data into three sets \n#### When selecting a model, you want to choose one that performs well both on the training and cross validation set. It implies that it is able to learn the patterns from your training set without overfitting","metadata":{}},{"cell_type":"code","source":"# Get 60% of the dataset as the training set. Put the remaining 40% in temporary variables: x_ and y_.\nx_train, x_, y_train, y_ = train_test_split(X, y, test_size=0.40, random_state=1)\n\n# Split the 40% subset above into two: one half for cross validation and the other for the test set\nx_cv, x_test, y_cv, y_test = train_test_split(x_, y_, test_size=0.50, random_state=1)\n\n# After splitting the 3 data sets, make sure to normalize(z-score) them all (using the same Scaler for the training data set) if you plan on training the model with the normalized training set\n# when using the z-score is you have to use the mean and standard deviation of the training set when scaling the cross validation set. This is to ensure that your input features are transformed as expected by the model. \n# You will scale the cross validation set  by using the same StandardScaler you used earlier for training but only calling its transform() method instead of fit_transform()\n\nscaler = StandardScaler()\n\nX_norm = scaler.fit_transform(x_train) \n\n# Notice that you are using the mean and standard deviation computed from the training set by just using transform() in the cross validation and test sets instead of fit_transform()\nx_cv_norm = scaler.transform(x_cv)\n\nx_test_norm = scaler.transform(x_test)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LR\n#### Choosing form different Models with different polynomial features (up to 10)\n#### We will chose the model with the lowest MSE\n#### The first degree is 1 to indicate that it will just use x_train, x_cv, and x_test as is (i.e. without any additional polynomial features).","metadata":{}},{"cell_type":"code","source":"# Initialize lists to save the errors, models, and feature transforms\ntrain_mses = []\ncv_mses = []\nmodels = []\npolys = []\nscalers = []\n\n# Loop over 10 times. Each adding one more degree of polynomial higher than the last.\nfor degree in range(1,11):\n    \n    # Add polynomial features to the training set (Will create a new input feature x^degree)\n    poly = PolynomialFeatures(degree, include_bias=False)\n    X_train_mapped = poly.fit_transform(x_train)\n    polys.append(poly)\n    \n    # Scale the training set\n    scaler_poly = StandardScaler()\n    X_train_mapped_scaled = scaler_poly.fit_transform(X_train_mapped)\n    scalers.append(scaler_poly)\n    \n    # Create and train the model\n    model = LinearRegression()\n    model.fit(X_train_mapped_scaled, y_train )\n    models.append(model)\n    \n    # Compute the training MSE\n    yhat = model.predict(X_train_mapped_scaled)\n    train_mse = mean_squared_error(y_train, yhat) / 2\n    train_mses.append(train_mse)\n    \n    # Add polynomial features and scale the cross validation set\n    X_cv_mapped = poly.transform(x_cv)\n    X_cv_mapped_scaled = scaler_poly.transform(X_cv_mapped)\n    \n    # Compute the cross validation MSE\n    yhat = model.predict(X_cv_mapped_scaled)\n    cv_mse = mean_squared_error(y_cv, yhat) / 2\n    cv_mses.append(cv_mse)\n\n    \n# Get the model with the lowest CV MSE (add 1 because list indices start at 0)\n# This also corresponds to the degree of the polynomial added\ndegree = np.argmin(cv_mses) + 1\nprint(f\"Lowest CV MSE is found in the model with degree={degree}\")\n\n\n\n# Add polynomial features to the test set\nX_test_mapped = polys[degree-1].transform(x_test)\n\n# Scale the test set\nX_test_mapped_scaled = scalers[degree-1].transform(X_test_mapped)\n\n# Compute the test MSE\nyhat = models[degree-1].predict(X_test_mapped_scaled)\ntest_mse = mean_squared_error(y_test, yhat) / 2\n\nprint(f\"Training MSE: {train_mses[degree-1]:.2f}\")\nprint(f\"Cross Validation MSE: {cv_mses[degree-1]:.2f}\")\nprint(f\"Test MSE: {test_mse:.2f}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## NN\n#### The same model selection process can also be used when choosing between different neural network architectures\n#### Neural networks can learn non-linear relationships so you can opt to skip adding polynomial features","metadata":{}},{"cell_type":"code","source":"# Preparing the data\n# Add polynomial features\ndegree = 1\npoly = PolynomialFeatures(degree, include_bias=False)\nX_train_mapped = poly.fit_transform(x_train)\nX_cv_mapped = poly.transform(x_cv)\nX_test_mapped = poly.transform(x_test)\n\n# Scale the features using the z-score\nscaler = StandardScaler()\nX_train_mapped_scaled = scaler.fit_transform(X_train_mapped)\nX_cv_mapped_scaled = scaler.transform(X_cv_mapped)\nX_test_mapped_scaled = scaler.transform(X_test_mapped)\n\n# Initialize lists that will contain the errors for each model\nnn_train_mses = []\nnn_cv_mses = []\n\n# Build the models (Different Sequentials with different architectures saved in this list to iterate over)\nnn_models = utils.build_models()\n\n# Loop over the the models\nfor model in nn_models:\n    \n    # Setup the loss and optimizer\n    model.compile(\n    loss='mse',\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n    )\n\n    print(f\"Training {model.name}...\")\n    \n    # Train the model\n    model.fit(\n        X_train_mapped_scaled, y_train,\n        epochs=300,\n        verbose=0\n    )\n    \n    print(\"Done!\\n\")\n\n    \n    # Record the training MSEs\n    yhat = model.predict(X_train_mapped_scaled)\n    train_mse = mean_squared_error(y_train, yhat) / 2\n    nn_train_mses.append(train_mse)\n    \n    # Record the cross validation MSEs \n    yhat = model.predict(X_cv_mapped_scaled)\n    cv_mse = mean_squared_error(y_cv, yhat) / 2\n    nn_cv_mses.append(cv_mse)\n\n    \n# print results\nprint(\"RESULTS:\")\nfor model_num in range(len(nn_train_mses)):\n    print(\n        f\"Model {model_num+1}: Training MSE: {nn_train_mses[model_num]:.2f}, \" +\n        f\"CV MSE: {nn_cv_mses[model_num]:.2f}\"\n        )\n    \n    \n# Get the model with the lowest CV MSE (add 1 because list indices start at 0)\nmodel_num = np.argmin(nn_cv_mses) + 1\nprint(f\"Lowest CV MSE is found in the model with degree={degree}\")\n\n\n# Compute the test MSE\nyhat = nn_models[model_num-1].predict(X_test_mapped_scaled)\ntest_mse = mean_squared_error(y_test, yhat) / 2\n\nprint(f\"Selected Model: {model_num}\")\nprint(f\"Training MSE: {nn_train_mses[model_num-1]:.2f}\")\nprint(f\"Cross Validation MSE: {nn_cv_mses[model_num-1]:.2f}\")\nprint(f\"Test MSE: {test_mse:.2f}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Classification\n#### In the previous sections on regression models, you used the mean squared error to measure how well your model is doing. For classification, you can get a similar metric by getting the fraction of the data that the model has misclassified. ","metadata":{}},{"cell_type":"code","source":"# Load the dataset from a text file\ndata = np.loadtxt('./data/data_w3_ex2.csv', delimiter=',')\n\n# Split the inputs and outputs into separate arrays\nx_bc = data[:,:-1]\ny_bc = data[:,-1]\n\n# Convert y into 2-D because the commands later will require it (x is already 2-D)\ny_bc = np.expand_dims(y_bc, axis=1)\n\n\n# Get 60% of the dataset as the training set. Put the remaining 40% in temporary variables.\nx_bc_train, x_, y_bc_train, y_ = train_test_split(x_bc, y_bc, test_size=0.40, random_state=1)\n\n# Split the 40% subset above into two: one half for cross validation and the other for the test set\nx_bc_cv, x_bc_test, y_bc_cv, y_bc_test = train_test_split(x_, y_, test_size=0.50, random_state=1)\n\n\n# Scale the features\n\n# Initialize the class\nscaler_linear = StandardScaler()\n\n# Compute the mean and standard deviation of the training set then transform it\nx_bc_train_scaled = scaler_linear.fit_transform(x_bc_train)\nx_bc_cv_scaled = scaler_linear.transform(x_bc_cv)\nx_bc_test_scaled = scaler_linear.transform(x_bc_test)\n\n\n# Sample model output\nprobabilities = np.array([0.2, 0.6, 0.7, 0.3, 0.8])\n\n# Apply a threshold to the model output. If greater than 0.5, set to 1. Else 0.\npredictions = np.where(probabilities >= 0.5, 1, 0)\n\n# Ground truth labels\nground_truth = np.array([1, 1, 1, 1, 1])\n\n# Initialize counter for misclassified data\nmisclassified = 0\n\n# Get number of predictions\nnum_predictions = len(predictions)\n\n# Loop over each prediction\nfor i in range(num_predictions):\n    \n    # Check if it matches the ground truth\n    if predictions[i] != ground_truth[i]:\n        \n        # Add one to the counter if the prediction is wrong\n        misclassified += 1\n\n# Compute the fraction of the data that the model misclassified\nfraction_error = misclassified/num_predictions\n\nprint(f\"probabilities: {probabilities}\")\nprint(f\"predictions with threshold=0.5: {predictions}\")\nprint(f\"targets: {ground_truth}\")\nprint(f\"fraction of misclassified data (for-loop): {fraction_error}\")\nprint(f\"fraction of misclassified data (with np.mean()): {np.mean(predictions != ground_truth)}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Using NN \n# Initialize lists that will contain the errors for each model\nnn_train_error = []\nnn_cv_error = []\n\n# Build the models\nmodels_bc = utils.build_models()\n\n# Loop over each model\nfor model in models_bc:\n    \n    # Setup the loss and optimizer\n    model.compile(\n    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n    )\n\n    print(f\"Training {model.name}...\")\n\n    # Train the model\n    model.fit(\n        x_bc_train_scaled, y_bc_train,\n        epochs=200,\n        verbose=0\n    )\n    \n    print(\"Done!\\n\")\n    \n    # Set the threshold for classification\n    threshold = 0.5\n    \n    # Record the fraction of misclassified examples for the training set\n    yhat = model.predict(x_bc_train_scaled)\n    yhat = tf.math.sigmoid(yhat)\n    yhat = np.where(yhat >= threshold, 1, 0)\n    train_error = np.mean(yhat != y_bc_train)\n    nn_train_error.append(train_error)\n\n    # Record the fraction of misclassified examples for the cross validation set\n    yhat = model.predict(x_bc_cv_scaled)\n    yhat = tf.math.sigmoid(yhat)\n    yhat = np.where(yhat >= threshold, 1, 0)\n    cv_error = np.mean(yhat != y_bc_cv)\n    nn_cv_error.append(cv_error)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the result\n# If there is a tie on the cross validation set error, then you can add another criteria to break it. For example, you can choose the one with a lower training error. A more common approach is to choose the smaller model because it saves computational resources.\nfor model_num in range(len(nn_train_error)):\n    print(\n        f\"Model {model_num+1}: Training Set Classification Error: {nn_train_error[model_num]:.5f}, \" +\n        f\"CV Set Classification Error: {nn_cv_error[model_num]:.5f}\"\n        )\n    \n # Get the model with the lowest CV MSE (add 1 because list indices start at 0)\nmodel_num = np.argmin(nn_cv_mses) + 1\nprint(f\"Lowest CV MSE is found in the model with degree={degree}\")\n# Compute the test error\nyhat = models_bc[model_num-1].predict(x_bc_test_scaled)\nyhat = tf.math.sigmoid(yhat)\nyhat = np.where(yhat >= threshold, 1, 0)\nnn_test_error = np.mean(yhat != y_bc_test)\n\nprint(f\"Selected Model: {model_num}\")\nprint(f\"Training Set Classification Error: {nn_train_error[model_num-1]:.4f}\")\nprint(f\"CV Set Classification Error: {nn_cv_error[model_num-1]:.4f}\")\nprint(f\"Test Set Classification Error: {nn_test_error:.4f}\")","metadata":{},"execution_count":null,"outputs":[]}]}